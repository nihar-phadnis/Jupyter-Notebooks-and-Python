{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1: Train a Sentiment Analysis Classifier\n",
    "In this course work, you are asked to train a sentiment analysis classifier for movie reviews. Below using appropriate machine learning based NLP techniques, an analysis is performed over a given text corpus to train the model. First we read the dataset & it has been done using Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Enjoy the opening credits. They're the best th...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Well, the Sci-Fi channel keeps churning these ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>It takes guts to make a movie on Gandhi in Ind...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Nest is really just another 'nature run am...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Waco: Rules of Engagement does a very good job...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text sentiment\n",
       "0           0  Enjoy the opening credits. They're the best th...       neg\n",
       "1           1  Well, the Sci-Fi channel keeps churning these ...       neg\n",
       "2           2  It takes guts to make a movie on Gandhi in Ind...       pos\n",
       "3           3  The Nest is really just another 'nature run am...       neg\n",
       "4           4  Waco: Rules of Engagement does a very good job...       pos"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data and take a quick look\n",
    "import pandas as pd\n",
    "raw_data = pd.read_csv('reviews.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 40000 rows in the dataset & 3 columnswith the last one consisting whether the corresponding review is negative or positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try exploring how a bit of data looks like here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Enjoy the opening credits. They're the best th...\n",
       "1        Well, the Sci-Fi channel keeps churning these ...\n",
       "2        It takes guts to make a movie on Gandhi in Ind...\n",
       "3        The Nest is really just another 'nature run am...\n",
       "4        Waco: Rules of Engagement does a very good job...\n",
       "                               ...                        \n",
       "39995    This was a Hindi movie. Hindi=Horrible. reason...\n",
       "39996    I'm really tempted to reward \"The Case of the ...\n",
       "39997    Poor Jane Austen. This dog of a production doe...\n",
       "39998    Subtle, delicate ,touching.<br /><br />A young...\n",
       "39999    OK, Anatomie is not a reinvention of the Horro...\n",
       "Name: text, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry num 40000\n",
      "num of pos entries 20000\n",
      "num of neg entries 20000\n"
     ]
    }
   ],
   "source": [
    "# check the size of the data and its class distribution\n",
    "all_text = raw_data['text'].tolist()\n",
    "all_lables = raw_data['sentiment'].tolist()\n",
    "\n",
    "print('entry num', len(all_text))\n",
    "print('num of pos entries', len([l for l in all_lables if l=='pos']))\n",
    "print('num of neg entries', len([l for l in all_lables if l=='neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now below we're going to pre-process the text so that it can be analysed. Following packages have to be imported in order to perform the pre-processing/data cleaning step. \n",
    "\n",
    "Stopwords like “the”, “a”, “an”, “in” which don't affect the meaning of the sentence are removed from using this package. Also, the word_tokenize will help to tokenize each & every word. & Stemming is preferred over Lemmatization as it was giving more faster results by computational purposes and also did not let down the accuracy by much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning and preprocessing:\n",
    "# This sample code does not perform any text normalization/pre-processing\n",
    "# Feel free to apply any pre-processing steps you find appropriate\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_txt(txt):\n",
    "    corpus = []    \n",
    "    for t in txt:\n",
    "        \n",
    "        #First it is converted to lower and anyother than alphabets are removed, also punctuations are also removed.\n",
    "        \n",
    "        review = t.lower()\n",
    "        review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "        review = re.sub(r'[^\\w\\s]', '', review)\n",
    "        \n",
    "        w_txt = word_tokenize(review)\n",
    "        words = [ww for ww in w_txt if ww not in stopwords]\n",
    "        detok = ' '.join(words)\n",
    "        corpus.append(detok)\n",
    "    \n",
    "    return corpus \n",
    "\n",
    "clean = clean_txt(raw_data.text)\n",
    "#clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here splitting of data is done using the scikit learn model selection train_test_split. I chose to do it with this as I was familiar with this approach from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split. \n",
    "# Feel free to use differnt raios or strategies to split the data.\n",
    "#train_text = all_text[:35000]\n",
    "#train_labels = all_lables[:35000]\n",
    "#test_text = all_text[35000:]\n",
    "#test_labels = all_lables[35000:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(clean,raw_data.sentiment, random_state=220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis classifier: Here, various machine learning algorithms are tried and tested and depending upon the results, we'll choose the best one for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tf-idf vectorisation gives a matrix which in turn can tell how important a keyword is to the analysis of an sentiment. Which is why it is suitable for this type of code. \n",
    "\n",
    "Below:, three algorithms other than Logistic regression have been applied: Random Forest Classifier, KNeighbors classifier, Linear Discriminant analysis. Out of which in terms of accuracy, Logistic regression and the Linear Discriminant analysis are quite similar with LR edging by a bit. Computationally, LR was also the one which gave a faster output.\n",
    "\n",
    "After giving some parameters to the Random Forest Classifier, it's accuracy increased. And after using the technique of Grid Search, it gave '5' neighbors as the best parameters for KNeighbors classifiers. Although it gave a low acccuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.8658\n",
      "precision 0.865912122520385\n",
      "rec 0.8658490081570779\n",
      "f1 0.8657974018376996\n"
     ]
    }
   ],
   "source": [
    "# training: tf-idf + logistic regression\n",
    "# you should explore different representations and algorithms.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_feature_num = 1000\n",
    "train_vectorizer = TfidfVectorizer(max_features=max_feature_num)\n",
    "train_vecs = train_vectorizer.fit_transform(train_text)\n",
    "test_vecs = TfidfVectorizer(max_features=max_feature_num,vocabulary=train_vectorizer.vocabulary_).fit_transform(test_text)\n",
    "\n",
    "# train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression().fit(train_vecs, train_labels)\n",
    "\n",
    "# test model\n",
    "test_pred = clf.predict(test_vecs)\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "acc = accuracy_score(test_labels, test_pred)\n",
    "pre, rec, f1, _ = precision_recall_fscore_support(test_labels, test_pred, average='macro')\n",
    "print('acc', acc)\n",
    "print('precision', pre)\n",
    "print('rec', rec)\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.8352\n",
      "precision 0.835235326531\n",
      "rec 0.8352286752681708\n",
      "f1 0.8351998945279325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_feature_num = 1000\n",
    "train_vectorizer = TfidfVectorizer(max_features=max_feature_num)\n",
    "train_vecs = train_vectorizer.fit_transform(train_text)\n",
    "test_vecs = TfidfVectorizer(max_features=max_feature_num,vocabulary=train_vectorizer.vocabulary_).fit_transform(test_text)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 200, criterion = 'gini', random_state=220) \n",
    "\n",
    "clf.fit(train_vecs, train_labels)\n",
    "\n",
    "test_pred = clf.predict(test_vecs)\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "acc = accuracy_score(test_labels, test_pred)\n",
    "pre, rec, f1, _ = precision_recall_fscore_support(test_labels, test_pred, average='macro')\n",
    "print('acc', acc)\n",
    "print('precision', pre)\n",
    "print('rec', rec)\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.7143\n",
      "precision 0.7143429819899956\n",
      "rec 0.7143310898945814\n",
      "f1 0.7142989686192767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_feature_num = 1000\n",
    "train_vectorizer = TfidfVectorizer(max_features=max_feature_num)\n",
    "train_vecs = train_vectorizer.fit_transform(train_text)\n",
    "test_vecs = TfidfVectorizer(max_features=max_feature_num,vocabulary=train_vectorizer.vocabulary_).fit_transform(test_text)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5, metric= 'euclidean')\n",
    "\n",
    "clf.fit(train_vecs, train_labels)\n",
    "\n",
    "test_pred = clf.predict(test_vecs)\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "acc = accuracy_score(test_labels, test_pred)\n",
    "pre, rec, f1, _ = precision_recall_fscore_support(test_labels, test_pred, average='macro')\n",
    "print('acc', acc)\n",
    "print('precision', pre)\n",
    "print('rec', rec)\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.8616\n",
      "precision 0.8617885427359441\n",
      "rec 0.8616629260909248\n",
      "f1 0.8615932180676853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_feature_num = 1000\n",
    "train_vectorizer = TfidfVectorizer(max_features=max_feature_num)\n",
    "train_vecs = train_vectorizer.fit_transform(train_text)\n",
    "test_vecs = TfidfVectorizer(max_features=max_feature_num,vocabulary=train_vectorizer.vocabulary_).fit_transform(test_text)\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(train_vecs.toarray(), train_labels)\n",
    "\n",
    "test_pred = clf.predict(test_vecs)\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "acc = accuracy_score(test_labels, test_pred)\n",
    "pre, rec, f1, _ = precision_recall_fscore_support(test_labels, test_pred, average='macro')\n",
    "print('acc', acc)\n",
    "print('precision', pre)\n",
    "print('rec', rec)\n",
    "print('f1', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
