{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3- SVM, Cross- conformal predictors, Neural-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will see the implementation of Support vector machines, pipeline & Neural-Networks with the help of scikit-learn functions. Cross-conformal predictor has not been attempted. \n",
    "\n",
    "First, we will load the wine dataset & split it using the train_test_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = load_wine()\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest =  train_test_split(wine.data, wine.target, random_state= 2206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From sklearn, we import the cross-val score & use the default parameters to check the accuracy of the SVM.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score is: [0.66666667 0.59259259 0.7037037  0.61538462 0.61538462]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "svm = SVC()\n",
    "wineSVC = cross_val_score(svm, Xtrain, ytrain, n_jobs= -1)\n",
    "\n",
    "print('Cross val score is:', wineSVC)\n",
    "GenWine = np.mean(wineSVC)\n",
    "\n",
    "svm.fit(Xtrain, ytrain)\n",
    "\n",
    "testscore = svm.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs an array with each cross val score. As we can see from above, we get a varying score. So, we take the mean of the scores to get a generalised one which comes around to be: 0.6387. \n",
    "\n",
    "We also calculate the test error rate & it comes out to be 0.267. \n",
    "\n",
    "An accuracy of 26% with default parameters is quite good. We will tune the parameters later to have an even more optimal score. We also notice that the generalisation accuracy is quite low, as compared to the test error. \n",
    "\n",
    "Note: This is done with the default parameters to check the efficiency of our model. Later, we will normalise the data & use different parameters to see how it works on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalisation is: 0.6387464387464388\n",
      "The test score is: 0.7333333333333333\n",
      "Test error rate is: 0.2666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Generalisation is:', GenWine)\n",
    "print('The test score is:', testscore)\n",
    "print('Test error rate is:', 1-testscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will test the SVM with various pre-processing techniques. And observe which one works well. We will also use pipeline to apply normalisation and perfrom Cross val using GridSearchCV. \n",
    "\n",
    "First we will use MinMaxScaler along with different parameters of C & Gamma for our SVM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy:  0.9925925925925926\n",
      "Best parameters: {'svc__C': 0.1, 'svc__gamma': 1}\n",
      "Test score: 0.9555555555555556\n",
      "Test error score: 0.0444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "param_grid = {\"svc__C\": [0.1, 1, 10, 100,1000], \"svc__gamma\":[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "pipe = make_pipeline(MinMaxScaler(), SVC())\n",
    "grid = GridSearchCV(pipe, param_grid= param_grid, cv = 5)\n",
    "grid.fit(Xtrain, ytrain)\n",
    "score= grid.score(Xtest, ytest)\n",
    "\n",
    "print('Best cross-validation accuracy: ', grid.best_score_)\n",
    "print('Best parameters:', grid.best_params_)\n",
    "print('Test score:', score)\n",
    "print('Test error score:', round((1-score),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use normalisation technique- Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy:  0.962962962962963\n",
      "Best parameters: {'svc__C': 1000, 'svc__gamma': 100}\n",
      "Test score: 0.8666666666666667\n",
      "Test error score: 0.1333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "param_grid = {\"svc__C\": [0.1, 1, 10, 100, 1000], \"svc__gamma\":[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "pipe = make_pipeline(Normalizer(), SVC())\n",
    "grid = GridSearchCV(pipe, param_grid= param_grid, cv = 5)\n",
    "grid.fit(Xtrain, ytrain)\n",
    "score= grid.score(Xtest, ytest)\n",
    "\n",
    "print('Best cross-validation accuracy: ', grid.best_score_)\n",
    "print('Best parameters:', grid.best_params_)\n",
    "print('Test score:', score)\n",
    "print('Test error score:', round((1-score),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now: StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy:  0.9925925925925926\n",
      "Best parameters: {'svc__C': 1, 'svc__gamma': 0.01}\n",
      "Test score: 0.9555555555555556\n",
      "Test error score: 0.0444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "param_grid = {\"svc__C\": [0.1, 1, 10, 100, 1000], \"svc__gamma\":[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), SVC())\n",
    "grid = GridSearchCV(pipe, param_grid= param_grid, cv = 5)\n",
    "grid.fit(Xtrain, ytrain)\n",
    "score= grid.score(Xtest, ytest)\n",
    "\n",
    "print('Best cross-validation accuracy: ', grid.best_score_)\n",
    "print('Best parameters:', grid.best_params_)\n",
    "print('Test score:', score)\n",
    "print('Test error score:', round((1-score),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can observe that the Standardization technique StandardScaler & scaling MinMaxScaler give similar type of results. And when compared to the normalisation technique- Normaliser; we see that the test score and test error rates are quite as compared to other two pre-processing tecniques. This difference might due to the fact it is normalising the data initially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we import the MLPClassifier for neural network on this Wine dataset. First we check our model on the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross-val: [0.59259259 0.88888889 0.33333333 0.92307692 0.38461538]\n",
      "Generalisation score is: 0.6245014245014244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "score= cross_val_score(mlp, Xtrain, ytrain)\n",
    "print('Using cross-val:', score)\n",
    "\n",
    "scores= np.mean(score)\n",
    "\n",
    "print('Generalisation score is:', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error 0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(Xtrain, ytrain)\n",
    "\n",
    "testscore = mlp.score(Xtest, ytest)\n",
    "print('Test error', 1-testscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the test error and the generalisation score, we notice the accuracy is better than the SVC & at the same time the test error rate is also low as compared to our SVC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce different pre-processing types along with the tuned parameters of the MLPClassifier & compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy:  0.9851851851851852\n",
      "Best parameters: {'mlpclassifier__alpha': 0.01, 'mlpclassifier__beta_1': 0.9}\n",
      "Test score: 0.9777777777777777\n",
      "Test error score: 0.0222\n"
     ]
    }
   ],
   "source": [
    "mlp1= MLPClassifier(solver= 'lbfgs', hidden_layer_sizes= [10])\n",
    "param_grid = {'mlpclassifier__alpha': [0.01, 0.1, 1, 10, 100], 'mlpclassifier__beta_1':[0.09, 0.9, 0.999, 9]}\n",
    "\n",
    "pipe1 = make_pipeline(MinMaxScaler(), mlp1)\n",
    "grid1 = GridSearchCV(pipe1, param_grid= param_grid, cv = 5)\n",
    "grid1.fit(Xtrain, ytrain)\n",
    "score= grid1.score(Xtest, ytest)\n",
    "\n",
    "print('Best cross-validation accuracy: ', grid1.best_score_)\n",
    "print('Best parameters:', grid1.best_params_)\n",
    "print('Test score:', score)\n",
    "print('Test error score:', round((1-score),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy:  1.0\n",
      "Best parameters: {'mlpclassifier__alpha': 10, 'mlpclassifier__beta_1': 0.09}\n",
      "Test score: 0.9777777777777777\n",
      "Test error score: 0.0222\n"
     ]
    }
   ],
   "source": [
    "pipe1 = make_pipeline(StandardScaler(), mlp1)\n",
    "grid1 = GridSearchCV(pipe1, param_grid= param_grid, cv = 5)\n",
    "grid1.fit(Xtrain, ytrain)\n",
    "score= grid1.score(Xtest, ytest)\n",
    "\n",
    "print('Best cross-validation accuracy: ', grid1.best_score_)\n",
    "print('Best parameters:', grid1.best_params_)\n",
    "print('Test score:', score)\n",
    "print('Test error score:', round((1-score),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy:  0.9242165242165242\n",
      "Best parameters: {'mlpclassifier__alpha': 0.01, 'mlpclassifier__beta_1': 0.9}\n",
      "Test score: 0.6888888888888889\n",
      "Test error score: 0.3111\n"
     ]
    }
   ],
   "source": [
    "pipe1 = make_pipeline(Normalizer(), mlp1)\n",
    "grid1 = GridSearchCV(pipe1, param_grid= param_grid, cv = 5)\n",
    "grid1.fit(Xtrain, ytrain)\n",
    "score= grid1.score(Xtest, ytest)\n",
    "\n",
    "print('Best cross-validation accuracy: ', grid1.best_score_)\n",
    "print('Best parameters:', grid1.best_params_)\n",
    "print('Test score:', score)\n",
    "print('Test error score:', round((1-score),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare that the MinMaxScaler & StandardScaler give us the best test error score. Normaliser gives us an error rate of ~0.134. Although their results(MinMaxScaler & StandardScaler) are similar except cross-val accuracy, the best parameters chosen are different. \n",
    "\n",
    "Also, we used different parameters for MLPClassifier instead of only the default ones. We used the solver 'lbfgs' instead of default 'Adam' as lbfgs works well on smaller dataset & converges faster too. At the same time, we have mentioned the hidden layers to be [10] instead of default 100 as it's a smaller dataset. \n",
    "\n",
    "Note: While I used 'lbfgs', I had experimented with the default 'Adam' too initially, & it took a lot more iterations to converge as compared to 'lbfgs' when it compared quite faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USPS DATASET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the genfromtxt to extract the train and test sets. We then concatenate them to have a dataset as a whole and again split it using train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7291, 257)\n",
      "(2007, 257)\n"
     ]
    }
   ],
   "source": [
    "train = np.genfromtxt('zip.train')\n",
    "test = np.genfromtxt('zip.test')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9298, 257)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "usps = numpy.concatenate([train, test], axis= 0)\n",
    "print(usps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(usps[:,1:], usps[:,0], random_state = 2206)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the SVC on USPS dataset with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross-val with default parameters: [0.96917563 0.96774194 0.97562724 0.97202296 0.97345768]\n",
      "Generalisation score is:  0.9716050868288569\n"
     ]
    }
   ],
   "source": [
    "svm  = SVC()\n",
    "\n",
    "score = cross_val_score(svm, X_train, y_train)\n",
    "\n",
    "print('Using cross-val with default parameters:', score)\n",
    "\n",
    "print('Generalisation score is: ', (np.mean(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error score: 0.0241\n"
     ]
    }
   ],
   "source": [
    "svm.fit(X_train, y_train)\n",
    "\n",
    "testscoreUSPS = svm.score(X_test, y_test)\n",
    "\n",
    "print('Test error score:', round((1-testscoreUSPS),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the generalisation score and the test error score is very good for the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the different parameters of SVC on the USPS dataset. As the dataset takes too much time to compute, only StandardScaler & Normaliser is used here. \n",
    "\n",
    "The cross val accuracy using the default parameters is better as compared to the different parameters of C & Gamma and StandardScaler. Also, here, Normaliser gives us a better result as compared to the StandardScaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-val accuracy: 0.9211242512291871\n",
      "Best parameters: {'svc__C': 1, 'svc__gamma': 0.01}\n",
      "Test score: 0.9294623655913978\n",
      "Test error rate 0.0705\n"
     ]
    }
   ],
   "source": [
    "param_gridUSPS = {'svc__C': [0.01, 0.1, 1], 'svc__gamma':[0.01, 0.1, 1]}\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), svm)\n",
    "gridUSPS= GridSearchCV(pipe, param_grid=param_gridUSPS, cv = 3, n_jobs= -1)\n",
    "gridUSPS.fit(X_train, y_train)\n",
    "score = gridUSPS.score(X_test, y_test)\n",
    "\n",
    "print('Best cross-val accuracy:', gridUSPS.best_score_)\n",
    "print('Best parameters:', gridUSPS.best_params_)\n",
    "print('Test score:', score)\n",
    "print('Test error rate',  round((1-score),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error rate 0.0705\n"
     ]
    }
   ],
   "source": [
    "print('Test error rate',  round((1-score),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-val accuracy: 0.9660124985348459\n",
      "Best parameters: {'svc__C': 1, 'svc__gamma': 1}\n",
      "Test score: 0.9716129032258064\n",
      "Test error rate 0.0284\n"
     ]
    }
   ],
   "source": [
    "param_gridUSPS = {'svc__C': [0.01, 0.1, 1], 'svc__gamma':[0.01, 0.1, 1]}\n",
    "\n",
    "pipe = make_pipeline(Normalizer(), svm)\n",
    "gridUSPS= GridSearchCV(pipe, param_grid=param_gridUSPS, cv = 3, n_jobs= -1)\n",
    "gridUSPS.fit(X_train, y_train)\n",
    "score1 = gridUSPS.score(X_test, y_test)\n",
    "\n",
    "print('Best cross-val accuracy:', gridUSPS.best_score_)\n",
    "print('Best parameters:', gridUSPS.best_params_)\n",
    "print('Test score:', score1)\n",
    "print('Test error rate',  round((1-score1),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use a MLPClassifier on the USPS Dataset. This time we have also used MinMaxScaler. The results are very close between the three. MinMaxScaler & StandardScaler are quite similar again with different values of alpha and beta chosen by them. \n",
    "\n",
    "Note: This time we've used the default parameters as it's a large dataset so it works well here.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-val accuracy: 0.955113294961721\n",
      "Best parameters: {'mlpclassifier__alpha': 0.01, 'mlpclassifier__beta_1': 0.9}\n",
      "Test score: 0.970752688172043\n",
      "Test error rate 0.0292\n"
     ]
    }
   ],
   "source": [
    "mlpUS = MLPClassifier()\n",
    "\n",
    "param_gridUSPS = {'mlpclassifier__alpha': [0.01, 0.1, 1], 'mlpclassifier__beta_1':[0.09, 0.9]}\n",
    "\n",
    "pipe = make_pipeline(Normalizer(), mlpUS)\n",
    "gridUSPS= GridSearchCV(pipe, param_grid=param_gridUSPS, cv = 3, n_jobs= -1)\n",
    "gridUSPS.fit(X_train, y_train)\n",
    "score2 = gridUSPS.score(X_test, y_test)\n",
    "\n",
    "print('Best cross-val accuracy:', gridUSPS.best_score_)\n",
    "print('Best parameters:', gridUSPS.best_params_)\n",
    "print('Test score:', score2)\n",
    "print('Test error rate',  round((1-score2),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-val accuracy: 0.9644354376029464\n",
      "Best parameters: {'mlpclassifier__alpha': 0.1, 'mlpclassifier__beta_1': 0.09}\n",
      "Test score: 0.9733333333333334\n",
      "Test error rate 0.0267\n"
     ]
    }
   ],
   "source": [
    "mlpUS = MLPClassifier()\n",
    "\n",
    "param_gridUSPS = {'mlpclassifier__alpha': [0.01, 0.1, 1], 'mlpclassifier__beta_1':[0.09, 0.9]}\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), mlpUS)\n",
    "gridUSPS= GridSearchCV(pipe, param_grid=param_gridUSPS, cv = 3, n_jobs= -1)\n",
    "gridUSPS.fit(X_train, y_train)\n",
    "score3 = gridUSPS.score(X_test, y_test)\n",
    "\n",
    "print('Best cross-val accuracy:', gridUSPS.best_score_)\n",
    "print('Best parameters:', gridUSPS.best_params_)\n",
    "print('Test score:', score3)\n",
    "print('Test error rate',  round((1-score3),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation accuracy:  0.9611370828937872\n",
      "Best parameters: {'mlpclassifier__alpha': 0.1, 'mlpclassifier__beta_1': 0.9}\n",
      "Test score: 0.9711827956989247\n",
      "Test error score: 0.0288\n"
     ]
    }
   ],
   "source": [
    "param_gridUSPS = {'mlpclassifier__alpha': [0.01, 0.1, 1], 'mlpclassifier__beta_1':[0.09, 0.9]}\n",
    "\n",
    "\n",
    "pipe = make_pipeline(MinMaxScaler(), mlpUS)\n",
    "gridUSPS = GridSearchCV(pipe, param_grid= param_gridUSPS, cv = 3, n_jobs= -1)\n",
    "gridUSPS.fit(X_train, y_train)\n",
    "score4= gridUSPS.score(X_test, y_test)\n",
    "\n",
    "print('Best cross-validation accuracy: ', gridUSPS.best_score_)\n",
    "print('Best parameters:', gridUSPS.best_params_)\n",
    "print('Test score:', score4)\n",
    "print('Test error score:', round((1-score4),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have applied the SVM & the Neural Networks on Wine and the USPS dataset along with different pre-processing techniques as well.\n",
    "\n",
    "The computation time required on a large dataset like USPS was quite long; so had to take less parameters and keep the number of CV low as well. Also, the use of n_jobs as described in the Appendix of the assignment might have to proved to be useful to the computation time too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
